{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07eb690d",
   "metadata": {},
   "source": [
    "# Qwen2.5-VL Demand Letter Spark Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a8b2274",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 3.5.0\n",
      "CUDA: True\n"
     ]
    }
   ],
   "source": [
    "# Cell 1\n",
    "import json\n",
    "import os\n",
    "import threading\n",
    "import uuid\n",
    "\n",
    "import fitz\n",
    "import numpy as np\n",
    "import torch\n",
    "from pydantic import BaseModel, Field\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "from qwen_vl_utils import process_vision_info\n",
    "from transformers import AutoProcessor, Qwen2_5_VLForConditionalGeneration\n",
    "\n",
    "spark = SparkSession.builder.appName(\"DemandLetterQwen2_5_VL\").getOrCreate()\n",
    "print(\"Spark version:\", spark.version)\n",
    "print(\"CUDA:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64871d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2\n",
    "class DemandLetterExtract(BaseModel):\n",
    "    is_demand_letter: bool|None=None\n",
    "    claim_number: str|None=None\n",
    "    claimant_name: str|None=None\n",
    "    claimant_address: str|None=None\n",
    "    claimant_contact_phone: str|None=None\n",
    "    claimant_contact_email: str|None=None\n",
    "    claimant_contact_facsimile: str|None=None\n",
    "    claimant_legal_office_information: str|None=None\n",
    "    insurance_company_representative: str|None=None\n",
    "    claim_amount: str|None=None\n",
    "    demand_letter_date: str|None=None\n",
    "    response_deadline_date: str|None=None\n",
    "    evidence_attached: list = Field(default_factory=list)\n",
    "    date_of_loss: str|None=None\n",
    "    insured_property_address: str|None=None\n",
    "    insured_asset_description: str|None=None\n",
    "    policy_number: str|None=None\n",
    "    referenced_policy_language: str|None=None\n",
    "    threats_of_legal_action: str|None=None\n",
    "    requested_resolution: str|None=None\n",
    "    tone_of_letter: str|None=None\n",
    "    claimant_stated_cause_of_loss: str|None=None\n",
    "    letter_response_markdown: str|None=None\n",
    "\n",
    "MASTER_PROMPT = \"\"\"You are an insurance claims adjuster analyzing a document containing both text and images (PDF page snapshots). \n",
    "Use ALL provided content — extracted text AND page images — to determine if the document is a demand letter and to extract structured data.  \n",
    "Images always override text in cases of conflict.\n",
    "\n",
    "Your job is to produce ONLY a JSON object matching the exact schema below, with no commentary, no Markdown, and no extra fields:\n",
    "\n",
    "{{\n",
    "    \"is_demand_letter\": boolean | false,\n",
    "    \"claim_number\": string | null,\n",
    "    \"claimant_name\": string | null,\n",
    "    \"claimant_address\": string | null,\n",
    "    \"claimant_contact_phone\": string | null,\n",
    "    \"claimant_contact_email\": string | null,\n",
    "    \"claimant_contact_facsimile\": string | null,\n",
    "    \"claimant_legal_office_information\": string | null,\n",
    "    \"insurance_company_representative\": string | null,\n",
    "    \"claim_amount\": string | null,\n",
    "    \"demand_letter_date\": string | null,\n",
    "    \"response_deadline_date\": string | null,\n",
    "    \"evidence_attached\": array,\n",
    "    \"date_of_loss\": string | null,\n",
    "    \"insured_property_address\": string | null,\n",
    "    \"insured_asset_description\": string | null,\n",
    "    \"policy_number\": string | null,\n",
    "    \"referenced_policy_language\": string | null,\n",
    "    \"threats_of_legal_action\": string | null,\n",
    "    \"requested_resolution\": string | null,\n",
    "    \"tone_of_letter\": string | null,\n",
    "    \"claimant_stated_cause_of_loss\": string | null,\n",
    "    \"letter_response_markdown\": string | null\n",
    "}}\n",
    "\n",
    "Rules:\n",
    "\n",
    "1. Use BOTH:  \n",
    "   - The extracted TEXT of the PDF pages  \n",
    "   - The IMAGE representations of the pages  \n",
    "   If there is any discrepancy, treat the IMAGE as the authoritative source.\n",
    "\n",
    "2. First determine whether the document IS a demand letter.  \n",
    "   If NOT a demand letter:  \n",
    "   - \"is_demand_letter\" = false  \n",
    "   - All other fields MUST be null (including \"letter_response_markdown\").  \n",
    "   - Return the JSON and stop.\n",
    "\n",
    "3. If it IS a demand letter:  \n",
    "   - Extract all fields strictly from the provided content.  \n",
    "   - Use null for any field not explicitly present.  \n",
    "   - Do not infer, guess, or hallucinate any information.\n",
    "\n",
    "4. For \"evidence_attached\":  \n",
    "   - Provide an array of evidence types ONLY if explicitly attached or referenced.  \n",
    "   - Otherwise leave as an empty array.\n",
    "\n",
    "5. The field \"letter_response_markdown\" must contain a short, professional acknowledgement letter from the insurance adjuster.  \n",
    "   Requirements:  \n",
    "   - No salutation (“Dear…”).  \n",
    "   - No names of adjusters.  \n",
    "   - Must acknowledge receipt of the demand.  \n",
    "   - Must state the claim is under review.  \n",
    "   - Must summarize ONLY verifiable facts present in the letter.  \n",
    "   - Must mention a 30-day review/response timeline.  \n",
    "   - No additional claims, legal commentary, or invented facts.\n",
    "\n",
    "6. Use consistent, neutral, factual language.  \n",
    "   Never fabricate legal, financial, or policy details.\n",
    "\n",
    "7. When reading images, pay attention to:  \n",
    "   - Headers (law office, claimant, insurer)  \n",
    "   - Stamps, dates, letterheads  \n",
    "   - Signatures  \n",
    "   - Embedded exhibits  \n",
    "   - Handwritten annotations  \n",
    "   - Policy numbers, claim numbers  \n",
    "   - Amounts demanded  \n",
    "\n",
    "8. Your output must be **strict JSON**, parsable with no trailing text or commentary.\n",
    "\n",
    "Now analyze the provided text and images and return ONLY the JSON object.\n",
    "\"\"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed675dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "\n",
    "def get_model_and_processor():\n",
    "    if not hasattr(get_model_and_processor, \"_model_instance\"):\n",
    "\n",
    "        from transformers import BitsAndBytesConfig\n",
    "\n",
    "        bnb = BitsAndBytesConfig(load_in_4bit=True)\n",
    "\n",
    "        model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "            \"Qwen/Qwen2.5-VL-7B-Instruct\",\n",
    "            torch_dtype=\"auto\",\n",
    "            device_map=\"balanced\",\n",
    "            quantization_config=bnb,\n",
    "        )\n",
    "\n",
    "        processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-7B-Instruct\")\n",
    "\n",
    "        setattr(get_model_and_processor, \"_model_instance\", model)\n",
    "        setattr(get_model_and_processor, \"_processor_instance\", processor)\n",
    "\n",
    "    return (\n",
    "        getattr(get_model_and_processor, \"_model_instance\"),\n",
    "        getattr(get_model_and_processor, \"_processor_instance\"),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec5b4586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4\n",
    "import io\n",
    "import uuid\n",
    "\n",
    "import fitz\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def extract_pdf_pages(pdf_path: str):\n",
    "    pdf = fitz.open(pdf_path)\n",
    "    out = []\n",
    "\n",
    "    for page in pdf:\n",
    "        # Extract text\n",
    "        text = page.get_text()\n",
    "\n",
    "        # Render at high DPI\n",
    "        pix = page.get_pixmap(dpi=200)\n",
    "\n",
    "        # Convert pixmap → PIL image\n",
    "        img = Image.open(io.BytesIO(pix.tobytes(\"png\")))\n",
    "\n",
    "        # Convert to grayscale (mode \"L\")\n",
    "        gray = img.convert(\"L\")\n",
    "\n",
    "        # Save grayscale PNG\n",
    "        fn = f\"/tmp/page_{uuid.uuid4()}.png\"\n",
    "        gray.save(fn)\n",
    "\n",
    "        out.append({\"text\": text, \"image_path\": fn})\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def analyze_pdf_local(pdf_path):\n",
    "    model, processor = get_model_and_processor()\n",
    "\n",
    "    pages = extract_pdf_pages(pdf_path)\n",
    "    all_outputs = []\n",
    "\n",
    "    for p in pages:\n",
    "        try:\n",
    "            # Build correct chat format for Qwen2.5-VL\n",
    "            msgs = [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"text\", \"text\": MASTER_PROMPT},\n",
    "                        {\"type\": \"image\", \"image\": p[\"image_path\"]},\n",
    "                        {\"type\": \"text\", \"text\": p[\"text\"]},\n",
    "                    ],\n",
    "                }\n",
    "            ]\n",
    "\n",
    "            # Prepare text template\n",
    "            prompt_text = processor.apply_chat_template(\n",
    "                msgs, tokenize=False, add_generation_prompt=True\n",
    "            )\n",
    "\n",
    "            # Extract vision tensors\n",
    "            imgs, vids = process_vision_info(msgs)\n",
    "\n",
    "            inputs = processor(\n",
    "                text=[prompt_text],\n",
    "                images=imgs,\n",
    "                videos=vids,\n",
    "                padding=True,\n",
    "                return_tensors=\"pt\",\n",
    "            ).to(model.device)\n",
    "\n",
    "            with torch.inference_mode():\n",
    "                gen = model.generate(**inputs, max_new_tokens=1024)\n",
    "\n",
    "            trimmed = [o[len(i) :] for i, o in zip(inputs.input_ids, gen)]\n",
    "            page_output = processor.batch_decode(trimmed, skip_special_tokens=True)[0]\n",
    "\n",
    "            all_outputs.append(page_output)\n",
    "\n",
    "        finally:\n",
    "            # Free VRAM between pages\n",
    "            # torch.cuda.empty_cache()\n",
    "            # torch.cuda.ipc_collect()\n",
    "            pass\n",
    "\n",
    "    # Final assembly → one JSON to parse\n",
    "    full_text = \"\\n\\n\".join(all_outputs)\n",
    "\n",
    "    return full_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3bfa864e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5\n",
    "def analyze_pdf_udf_impl(p):\n",
    "    try:\n",
    "        if p is None:\n",
    "            return json.dumps({\"error\": \"null path\"})\n",
    "        r = analyze_pdf_local(p)\n",
    "        return r\n",
    "    except Exception as e:\n",
    "        return json.dumps({\"error\": str(e), \"path\": p})\n",
    "\n",
    "\n",
    "analyze_pdf_udf = udf(analyze_pdf_udf_impl, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "533d40cb-ce10-420a-b99c-8c8d5e1999e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insurance_Demand_Letter.pdf\n"
     ]
    }
   ],
   "source": [
    "ls /data/docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c202424",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pdf_path</th>\n",
       "      <th>analysis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/data/docs/Insurance_Demand_Letter.pdf</td>\n",
       "      <td>```json\\n{\\n    \"is_demand_letter\": true,\\n   ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 pdf_path  \\\n",
       "0  /data/docs/Insurance_Demand_Letter.pdf   \n",
       "\n",
       "                                            analysis  \n",
       "0  ```json\\n{\\n    \"is_demand_letter\": true,\\n   ...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 6\n",
    "import glob\n",
    "\n",
    "pdf_files = glob.glob(\"/data/docs/*.pdf\")\n",
    "df = spark.createDataFrame([(f,) for f in pdf_files], [\"pdf_path\"])\n",
    "\n",
    "scored = df.withColumn(\"analysis\", analyze_pdf_udf(\"pdf_path\"))\n",
    "pandas_df = scored.toPandas()\n",
    "pandas_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1dd99352-99c0-44fa-ada7-a126265c37e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_df.to_csv('extracted_info.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
